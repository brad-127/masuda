・実行
python3 run.py
たまに上手く学習出来ないときがある（結果がおかしいとかではなく，学習手法の問題が原因そう）
学習が始まった後表示されるlogを最初の数学習ステップ分見て，nanが出てきていたら上手く行っていないので終了してもう一度実行
最初の数学習ステップでない場合は問題なし


・必要なライブラリ
自作環境を動かすには
pygame
pymunk
gym
など，他は一般的なライブラリ

強化学習には
tensorflow 1系
baselines  (openai系の強化学習を便利にするものが色々入ったライブラリ)
など必要
baselinesは入れるのが少し面倒だった記憶あり


・自作環境について
gym_armに自作環境が入っている
これをgym_arm/__init__.pyでgymの環境として登録して，run.pyの方でgym.make()で呼び出している
名称は'fingerL-v0'で，これは上記の__init__.pyに記述されている
対応するコードはfinger_L.py

python3 gym_arm/finger_L.py -r

で，ランダムアクションによる動作を表示ありで行なってくれる
自作環境が動くか試す時はこれだけを回すと分かりやすい


・学習に使う各触覚条件の変更について
rollouts.pyの125行目付近で，「tch」という変数に代入している
この変数はその後tfの予測モデルの学習の際に使われるものであり，ここを書き換えてどの触覚情報を学習に使うかを変更している
内側触覚を使用するとき：
tch = infos[0].get('touch')
外側触覚を使用するとき：
tch = infos[0].get('touch_out')
上腕触覚を使用するとき：
tch = infos[0].get('touch_first_arm')

・直接触覚信号を報酬として学習するとき
run.pyの引数の--ext_coeffと--int_coeffで，好奇心からくる内部報酬と環境からくる報酬の学習に使用する割合を変更出来る
普段の実験では好奇心モデルのみの報酬を使用するため，基本的に-ext_coeffは0,--int_coeffは1にして学習を行なっている
このとき，環境からくる報酬に触覚信号を渡してあげて，-ext_coeffを1,--int_coeffを0にすることで，環境からの報酬，つまり触覚信号のみで学習することが出来る
gym_arm/finger_L.pyの def step() のrewardに相当する二つ目の返り値に触覚信号(接触したら１，しなかったら0)を渡してそれのみで学習することで，単純に触覚反応を最大化させる行動を学習するようになる
変更点の流れ：
1. gym_arm/finger_L.pyのstep関数の二つ目の返り値を，使いたい触覚信号に変更する
dic["touch"][1] or dic["touch_out"][1] or dic["touch_first_arm"][1]
2. run.pyの引数の-ext_coeffを1,--int_coeffを0に変更する
これで実行すれば，指定した触覚を直接報酬信号として好奇心抜きで学習出来る



復元するとき：
run.pyの代わりにrestore.pyを回す(ただし，それ以外のコードは共有なので書き換えに注意)
restore_dataの中に復元用のデータを移すことで，そのデータを使用する

ここで使用する環境はfinger_L.pyではなくres_L.pyに書かれたEnvを使っている（通常の環境より，環境のデータを多く持ってくるプログラムになっている）

注意点：使用するデータの学習時の条件に合わせること（例えば，上腕触覚を使用して学習したデータなら上記のように上腕触覚を使用するrollout.pyになっているかをチェックする．もしくは学習時に環境の内部を変更したら，復元時に使用する環境であるres_L.pyの方も同様の変更を行う）









